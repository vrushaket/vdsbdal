{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca28beb8",
   "metadata": {},
   "source": [
    "Text Analytics\n",
    "\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a0831",
   "metadata": {},
   "source": [
    "### 1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13910f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77105779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/pict/.local/lib/python3.6/site-packages (3.6.7)\n",
      "Requirement already satisfied: tqdm in /home/pict/.local/lib/python3.6/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in /usr/lib/python3/dist-packages (from nltk) (0.11)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/pict/.local/lib/python3.6/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tqdm->nltk) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/pict/.local/lib/python3.6/site-packages (from importlib-resources->tqdm->nltk) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c76c9304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25745e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_text = docx2txt.process('testdoc.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28c8be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"Millions of people in India took part in an annual tree planting drive Sunday. More than 250 million saplings were planted in a single day across the country\\'s most-populous state.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b27a4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = re.sub(\"[^0-9a-zA-Z]+\", \" \", my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e8b36",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd7a83",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "379357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0ef1cba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pict/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "64548098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions of people in India took part in an annual tree planting drive Sunday More than 250 million saplings were planted in a single day across the country s most populous state']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pict/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tokenized_text = sent_tokenize(my_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad4972",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd4f72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2843969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions', 'of', 'people', 'in', 'India', 'took', 'part', 'in', 'an', 'annual', 'tree', 'planting', 'drive', 'Sunday', 'More', 'than', '250', 'million', 'saplings', 'were', 'planted', 'in', 'a', 'single', 'day', 'across', 'the', 'country', 's', 'most', 'populous', 'state']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(my_text)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eaa89e",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "009926b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3d50fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 3), ('Millions', 1)]\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist.most_common(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f477e1",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "64f9bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c57273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pict/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc2d4357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'who', 'won', 's', 'yourselves', 'haven', 'couldn', 'which', 'before', 'here', 'just', 'being', 'nor', 'only', \"you'd\", 'can', 'm', 'at', 'with', \"that'll\", 'they', 'into', \"doesn't\", \"hadn't\", \"haven't\", 'mustn', 'too', 'most', 'doesn', 'hers', 'out', 'yourself', \"hasn't\", 'am', 'in', 'over', 'down', 'been', \"needn't\", 'its', \"wouldn't\", \"mightn't\", \"couldn't\", 'now', \"mustn't\", 'shouldn', 'shan', 'where', 'needn', 'had', 'have', 'what', 'above', 'these', 'you', 'up', 'that', \"isn't\", 'once', 'don', 'by', \"should've\", 'will', \"shan't\", 'hasn', 'ourselves', 'he', 'were', 'o', 'wasn', 'weren', 'whom', 'for', 'was', \"you've\", 'a', \"aren't\", 'and', 'again', 'each', 'off', 'more', 'than', 'such', 'from', 'do', 'y', \"weren't\", 'on', 'didn', 'your', 'themselves', 'him', 'same', 'it', 'until', 'after', 'his', 'wouldn', 'through', 'own', 'theirs', 'other', 'should', \"you're\", 'himself', 'her', 'further', 'to', 'some', 'our', 've', 'all', 'having', \"you'll\", 'but', 'against', 'below', 'd', 'aren', 're', 'my', 'during', \"didn't\", 'while', 'how', \"she's\", 'itself', 'yours', 'does', 'then', 'did', 'when', 'both', \"won't\", 'is', 'not', 'i', 'an', 'hadn', 'those', \"shouldn't\", 'we', 't', \"wasn't\", 'she', 'there', 'very', 'isn', 'herself', 'mightn', 'are', 'the', 'no', 'if', 'so', \"don't\", 'ours', 'why', 'their', 'be', 'doing', 'because', 'of', 'between', 'under', 'them', 'or', 'ain', 'this', 'ma', \"it's\", 'few', 'as', 'll', 'has', 'myself', 'any', 'me', 'about'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6cd176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words:\n",
      " ['Millions', 'of', 'people', 'in', 'India', 'took', 'part', 'in', 'an', 'annual', 'tree', 'planting', 'drive', 'Sunday', 'More', 'than', '250', 'million', 'saplings', 'were', 'planted', 'in', 'a', 'single', 'day', 'across', 'the', 'country', 's', 'most', 'populous', 'state']\n",
      "\n",
      "\n",
      "Filterd Tokens:\n",
      " ['Millions', 'people', 'India', 'took', 'part', 'annual', 'tree', 'planting', 'drive', 'Sunday', 'More', '250', 'million', 'saplings', 'planted', 'single', 'day', 'across', 'country', 'populous', 'state']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []\n",
    "for w in tokenized_word:    \n",
    "    if w not in stop_words:\n",
    "         filtered_tokens.append(w)\n",
    "            \n",
    "print(\"Tokenized Words:\\n\",tokenized_word)\n",
    "print(\"\\n\\nFilterd Tokens:\\n\",filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f7f4b",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "254c01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2df23c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pict/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dab7c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " ['Millions', 'people', 'India', 'took', 'part', 'annual', 'tree', 'planting', 'drive', 'Sunday', 'More', '250', 'million', 'saplings', 'planted', 'single', 'day', 'across', 'country', 'populous', 'state']\n",
      "\n",
      "\n",
      "PoS tags\n",
      ": [('Millions', 'NNS'), ('people', 'NNS'), ('India', 'NNP'), ('took', 'VBD'), ('part', 'NN'), ('annual', 'JJ'), ('tree', 'NN'), ('planting', 'VBG'), ('drive', 'JJ'), ('Sunday', 'NNP'), ('More', 'JJR'), ('250', 'CD'), ('million', 'CD'), ('saplings', 'NNS'), ('planted', 'VBN'), ('single', 'JJ'), ('day', 'NN'), ('across', 'IN'), ('country', 'NN'), ('populous', 'JJ'), ('state', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos = pos_tag(filtered_tokens)\n",
    "print(\"Tokens:\\n\",filtered_tokens)\n",
    "print(\"\\n\\nPoS tags\\n:\",pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72e67b",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "13aa6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c89969c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ed69feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens After Removing Punctuations:\n",
      " ['Millions', 'people', 'India', 'took', 'part', 'annual', 'tree', 'planting', 'drive', 'Sunday', 'More', '250', 'million', 'saplings', 'planted', 'single', 'day', 'across', 'country', 'populous', 'state']\n",
      "\n",
      "\n",
      "Stemmed Tokens:\n",
      " ['million', 'peopl', 'india', 'took', 'part', 'annual', 'tree', 'plant', 'drive', 'sunday', 'more', '250', 'million', 'sapl', 'plant', 'singl', 'day', 'across', 'countri', 'popul', 'state']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words=[]\n",
    "\n",
    "for w in filtered_tokens:     \n",
    "     stemmed_words.append(ps.stem(w))\n",
    "        \n",
    "#reduces words to their word root word \n",
    "print(\"Filtered Tokens After Removing Punctuations:\\n\",filtered_tokens)\n",
    "print(\"\\n\\nStemmed Tokens:\\n\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff7905",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25b731f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fcea0dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pict/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "435e78ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/pict/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e56b0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "leemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad4636aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words:\n",
      " ['Millions', 'people', 'India', 'take', 'part', 'annual', 'tree', 'plant', 'drive', 'Sunday', 'More', '250', 'million', 'saplings', 'plant', 'single', 'day', 'across', 'country', 'populous', 'state']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_text = []\n",
    "for w in filtered_tokens:\n",
    "    lemmatized_text.append(leemmatizer.lemmatize(w,\"v\"))\n",
    "    \n",
    "#reduces words to their base word\n",
    "print(\"Lemmatized Words:\\n\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca2ca5a",
   "metadata": {},
   "source": [
    "## 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4348783",
   "metadata": {},
   "source": [
    "#### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d33eddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set()\n",
    "words_set = words_set.union(set(lemmatized_text))\n",
    "df = dict.fromkeys(words_set, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ab04421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in lemmatized_text:\n",
    "    df[word]= df[word]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a2e4206a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree': 1,\n",
       " 'plant': 2,\n",
       " 'saplings': 1,\n",
       " '250': 1,\n",
       " 'across': 1,\n",
       " 'day': 1,\n",
       " 'million': 1,\n",
       " 'single': 1,\n",
       " 'populous': 1,\n",
       " 'country': 1,\n",
       " 'annual': 1,\n",
       " 'people': 1,\n",
       " 'part': 1,\n",
       " 'Millions': 1,\n",
       " 'drive': 1,\n",
       " 'More': 1,\n",
       " 'Sunday': 1,\n",
       " 'take': 1,\n",
       " 'state': 1,\n",
       " 'India': 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a74236",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6918199",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = {}\n",
    "corpusCount = len(lemmatized_text)\n",
    "for word, count in df.items():\n",
    "    tf[word] = count / float(corpusCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d634adcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree': 0.047619047619047616,\n",
       " 'plant': 0.09523809523809523,\n",
       " 'saplings': 0.047619047619047616,\n",
       " '250': 0.047619047619047616,\n",
       " 'across': 0.047619047619047616,\n",
       " 'day': 0.047619047619047616,\n",
       " 'million': 0.047619047619047616,\n",
       " 'single': 0.047619047619047616,\n",
       " 'populous': 0.047619047619047616,\n",
       " 'country': 0.047619047619047616,\n",
       " 'annual': 0.047619047619047616,\n",
       " 'people': 0.047619047619047616,\n",
       " 'part': 0.047619047619047616,\n",
       " 'Millions': 0.047619047619047616,\n",
       " 'drive': 0.047619047619047616,\n",
       " 'More': 0.047619047619047616,\n",
       " 'Sunday': 0.047619047619047616,\n",
       " 'take': 0.047619047619047616,\n",
       " 'state': 0.047619047619047616,\n",
       " 'India': 0.047619047619047616}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fc62e",
   "metadata": {},
   "source": [
    "###  Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66eef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "113bb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfDict = {}\n",
    "N = len(df)\n",
    "idfDict = df;\n",
    "for word, val in idfDict.items():\n",
    "    idfDict[word] = math.log10(N / (float(val) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "34a226d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree': 1.0,\n",
       " 'plant': 1.0400268910004289,\n",
       " 'saplings': 1.0,\n",
       " '250': 1.0,\n",
       " 'across': 1.0,\n",
       " 'day': 1.0,\n",
       " 'million': 1.0,\n",
       " 'single': 1.0,\n",
       " 'populous': 1.0,\n",
       " 'country': 1.0,\n",
       " 'annual': 1.0,\n",
       " 'people': 1.0,\n",
       " 'part': 1.0,\n",
       " 'Millions': 1.0,\n",
       " 'drive': 1.0,\n",
       " 'More': 1.0,\n",
       " 'Sunday': 1.0,\n",
       " 'take': 1.0,\n",
       " 'state': 1.0,\n",
       " 'India': 1.0}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb58dc",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "21eb5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = {}\n",
    "for word, val in tf.items():\n",
    "    tfidf[word] = val * idfDict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b9a56470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree': 0.047619047619047616,\n",
       " 'plant': 0.09905018009527894,\n",
       " 'saplings': 0.047619047619047616,\n",
       " '250': 0.047619047619047616,\n",
       " 'across': 0.047619047619047616,\n",
       " 'day': 0.047619047619047616,\n",
       " 'million': 0.047619047619047616,\n",
       " 'single': 0.047619047619047616,\n",
       " 'populous': 0.047619047619047616,\n",
       " 'country': 0.047619047619047616,\n",
       " 'annual': 0.047619047619047616,\n",
       " 'people': 0.047619047619047616,\n",
       " 'part': 0.047619047619047616,\n",
       " 'Millions': 0.047619047619047616,\n",
       " 'drive': 0.047619047619047616,\n",
       " 'More': 0.047619047619047616,\n",
       " 'Sunday': 0.047619047619047616,\n",
       " 'take': 0.047619047619047616,\n",
       " 'state': 0.047619047619047616,\n",
       " 'India': 0.047619047619047616}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e2e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
